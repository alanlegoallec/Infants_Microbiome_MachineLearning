{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read parameters from command\n",
    "image_type = 'Liver'\n",
    "target = 'Age'\n",
    "model_name = 'DenseNet121'\n",
    "optimizer_name = 'Adam'\n",
    "learning_rate = 0.01\n",
    "\n",
    "#regularization: start with zero regularization. After good training performance AND overfitting is confirmed, use regularization.\n",
    "lam=0.0 #regularization: weight shrinking\n",
    "dropout_rate=0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###load libraries\n",
    "\n",
    "#read and write\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import tarfile\n",
    "import shutil\n",
    "import pyreadr\n",
    "import csv\n",
    "\n",
    "#maths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from math import sqrt\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "\n",
    "#images\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.ndimage import shift, rotate\n",
    "from skimage.color import gray2rgb\n",
    "\n",
    "#miscellaneous\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import gc\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten, Dense, Activation, Input, Reshape, BatchNormalization, InputLayer, Dropout, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Conv3D, MaxPooling3D, GlobalAveragePooling2D, LSTM\n",
    "from keras.models import Sequential, Model, model_from_json, clone_model\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, TensorBoard, ModelCheckpoint\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "#from tensorflow.keras.metrics import Recall, Precision, AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to put in helper file\n",
    "def append_ext(fn):\n",
    "    return fn+\".jpg\"\n",
    "\n",
    "def generate_base_model(model_name, lam, dropout_rate, import_weights):\n",
    "    if model_name in ['VGG16', 'VGG19']:\n",
    "        if model_name == 'VGG16':\n",
    "            from keras.applications.vgg16 import VGG16\n",
    "            base_model = VGG16(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'VGG19':\n",
    "            from keras.applications.vgg19 import VGG19\n",
    "            base_model = VGG19(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        x = base_model.output\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(4096, activation='relu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "        x = Dropout(dropout_rate)(x) \n",
    "    elif model_name in ['MobileNet', 'MobileNetV2']:\n",
    "        if model_name == 'MobileNet':\n",
    "            from keras.applications.mobilenet import MobileNet\n",
    "            base_model = MobileNet(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'MobileNetV2':\n",
    "            from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "            base_model = MobileNetV2(include_top=False, weights=import_weights, input_shape=(224,224,3))\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_name in ['DenseNet121', 'DenseNet169', 'DenseNet201']:\n",
    "        if model_name == 'DenseNet121':\n",
    "            from keras.applications.densenet import DenseNet121\n",
    "            base_model = DenseNet121(include_top=True, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'DenseNet169':\n",
    "            from keras.applications.densenet import DenseNet169\n",
    "            base_model = DenseNet169(include_top=True, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'DenseNet201':\n",
    "            from keras.applications.densenet import DenseNet201\n",
    "            base_model = DenseNet201(include_top=True, weights=import_weights, input_shape=(224,224,3))            \n",
    "        base_model = Model(base_model.inputs, base_model.layers[-2].output)\n",
    "        x = base_model.output\n",
    "    elif model_name in ['NASNetMobile', 'NASNetLarge']:\n",
    "        if model_name == 'NASNetMobile':\n",
    "            from keras.applications.nasnet import NASNetMobile\n",
    "            base_model = NASNetMobile(include_top=True, weights=import_weights, input_shape=(224,224,3))\n",
    "        elif model_name == 'NASNetLarge':\n",
    "            from keras.applications.nasnet import NASNetLarge\n",
    "            base_model = NASNetLarge(include_top=True, weights=import_weights, input_shape=(331,331,3))\n",
    "        base_model = Model(base_model.inputs, base_model.layers[-2].output)\n",
    "        x = base_model.output\n",
    "    elif model_name == 'Xception':\n",
    "        from keras.applications.xception import Xception\n",
    "        base_model = Xception(include_top=False, weights=import_weights, input_shape=(299,299,3))\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_name == 'InceptionV3':\n",
    "        from keras.applications.inception_v3 import InceptionV3\n",
    "        base_model = InceptionV3(include_top=False, weights=import_weights, input_shape=(299,299,3))\n",
    "        x = base_model.output        \n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    elif model_name == 'InceptionResNetV2':\n",
    "        from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "        base_model = InceptionResNetV2(include_top=False, weights=import_weights, input_shape=(299,299,3))\n",
    "        x = base_model.output        \n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "    return x, base_model.input\n",
    "\n",
    "def complete_architecture(x, input_shape, lam, dropout_rate):\n",
    "    x = Dense(1024, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(512, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(128, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(64, activation='selu', kernel_regularizer=regularizers.l2(lam))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    predictions = Dense(1, activation='linear')(x)\n",
    "    model = Model(inputs=input_shape, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "def R_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "  \n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "def set_learning_rate(model, optimizer_name, learning_rate):\n",
    "    opt = globals()[optimizer_name](lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error', metrics=[R_squared, root_mean_squared_error])\n",
    "    \n",
    "def initialize_history():\n",
    "    HISTORY = {}\n",
    "    for metric in ['loss'] + metrics:\n",
    "        for fold in folds_tune:\n",
    "            if(fold=='train'):\n",
    "                HISTORY[metric] = []\n",
    "            else:\n",
    "                HISTORY[fold + '_' + metric] = []\n",
    "    return HISTORY\n",
    "\n",
    "def update_history(HISTORY, history):\n",
    "    keys = history.history.keys()\n",
    "    for key in keys:\n",
    "        HISTORY[key] = HISTORY[key] + history.history[key]\n",
    "    return HISTORY\n",
    "    \n",
    "def plot_training(HISTORY, version):\n",
    "    keys = history.history.keys()\n",
    "    fig, axs = plt.subplots(1, int(len(keys)/2), sharey=False, sharex=True)\n",
    "    fig.set_figwidth(15)\n",
    "    fig.set_figheight(5)\n",
    "    epochs = np.array(range(len(HISTORY[metrics[0]])))\n",
    "    for i, metric in enumerate(['loss'] + metrics):\n",
    "        for key in [key for key in keys if metric in key][::-1]:\n",
    "            axs[i].plot(epochs, HISTORY[key])\n",
    "        axs[i].legend(['Training ' + metric, 'Validation ' + metric])\n",
    "        axs[i].set_title(metric + ' = f(Epoch)')\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "        axs[i].set_ylabel(metric)\n",
    "        axs[i].set_ylim((-0.2, 1.1))\n",
    "    #save figure as pdf\n",
    "    fig.savefig(\"../figures/Training_\" + version + '.pdf', bbox_inches='tight')\n",
    "    \n",
    "def save_model_weights(model, version):\n",
    "    model.save_weights(path_store + \"model_weights_\" + version + \".h5\")\n",
    "    print(\"Model's best weights for \"+ version + \" were saved.\")\n",
    "    \n",
    "def rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def generate_predictions_and_performances(model, GENERATORS, STEPSIZES):\n",
    "    PREDS={}\n",
    "    PERFORMANCES={}\n",
    "    for fold in folds:\n",
    "        generator = GENERATORS[fold]\n",
    "        generator.reset()\n",
    "        PREDS[fold]=model.predict_generator(generator, STEPSIZES[fold], verbose=1)\n",
    "    for metric in metrics:\n",
    "            PERFORMANCES[metric] = {}\n",
    "            for fold in folds:\n",
    "                PERFORMANCES[metric][fold] = metric_functions[metric](DATA_FEATURES[fold][target], PREDS[fold])\n",
    "    return PREDS, PERFORMANCES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters to put in helper file\n",
    "folds = ['train', 'val', 'test']\n",
    "folds_tune = ['train', 'val']\n",
    "models_names = ['VGG16', 'VGG19', 'MobileNet', 'MobileNetV2', 'DenseNet121', 'DenseNet169', 'DenseNet201', 'NASNetMobile', 'NASNetLarge', 'Xception', 'InceptionV3', 'InceptionResNetV2']\n",
    "images_sizes = ['224', '299', '331']\n",
    "metrics = ['R_squared', 'root_mean_squared_error']\n",
    "main_metrics = dict.fromkeys(['Age'], 'R_squared')\n",
    "main_metrics.update(dict.fromkeys(['Sex'], 'AUC'))\n",
    "metric_functions = {'R_squared':r2_score, 'root_mean_squared_error':rmse}\n",
    "\n",
    "#define dictionary to resize the images to the right size depending on the model\n",
    "input_size_models = dict.fromkeys(['VGG16', 'VGG19', 'MobileNet', 'MobileNetV2', 'DenseNet121', 'DenseNet169', 'DenseNet201', 'NASNetMobile'], 224)\n",
    "input_size_models.update(dict.fromkeys(['Xception', 'InceptionV3', 'InceptionResNetV2'], 299))\n",
    "input_size_models.update(dict.fromkeys(['NASNetLarge'], 331))\n",
    "\n",
    "#define dictionaries to format the text\n",
    "dict_folds={'train':'Training', 'val':'Validation', 'test':'Testing'}\n",
    "\n",
    "#define paths\n",
    "if '/Users/Alan/' in os.getcwd():\n",
    "    os.chdir('/Users/Alan/Desktop/Aging/Medical_Images/scripts/')\n",
    "    path_store = '../data/'\n",
    "    path_compute = '../data/'\n",
    "else:\n",
    "    os.chdir('/n/groups/patel/Alan/Aging/Medical_Images/scripts/')\n",
    "    path_store = '../data/'\n",
    "    path_compute = '/n/scratch2/al311/Aging/Medical_Images/data/'\n",
    "\n",
    "if image_type == 'Liver':\n",
    "    dir_images = path_store + '../../../../uk_biobank/main_data_52887/Liver/Liver_20204/'\n",
    "else:\n",
    "    sys.exit(\"Error. Image type not available\")\n",
    "\n",
    "#model\n",
    "image_size = input_size_models[model_name]\n",
    "import_weights = 'imagenet' #choose between None and 'imagenet'\n",
    "\n",
    "#compiler\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "continue_training = True\n",
    "main_metric = main_metrics[target]\n",
    "version = target + '_' + image_type + '_' + model_name + '_' + optimizer_name + '_' + str(learning_rate) + '_' + str(lam) + '_' + str(dropout_rate) + '_' + str(batch_size)\n",
    "\n",
    "#postprocessing\n",
    "boot_iterations=10000\n",
    "\n",
    "#set parameters\n",
    "random.seed(0)\n",
    "set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  1.13.1\n",
      "Build with Cuda :  True\n",
      "Gpu available :  True\n"
     ]
    }
   ],
   "source": [
    "#print versions and info\n",
    "print('tensorflow version : ', tf.__version__)\n",
    "print('Build with Cuda : ', tf.test.is_built_with_cuda())\n",
    "print('Gpu available : ', tf.test.is_gpu_available())\n",
    "#print('Available ressources : ', tf.config.experimental.list_physical_devices())\n",
    "config = tf.ConfigProto()\n",
    "#device_count = {'GPU': 1, 'CPU': mp.cpu_count() },log_device_placement =  True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess= tf.Session(config = config)\n",
    "K.set_session(session= sess)\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore which variables are present in the features dataset\n",
    "#data_features = pd.read_csv(path_store + \"../../../../uk_biobank/main_data_52887/ukb37397.csv\", nrows=1)\n",
    "#data_features.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the selected features\n",
    "data_features = pd.read_csv(path_store + \"../../../../uk_biobank/main_data_52887/ukb37397.csv\", usecols=['eid','21003-0.0','31-0.0', '22414-2.0'])\n",
    "data_features.columns = ['eid', 'Sex','Age', 'Data_quality']\n",
    "data_features['eid'] =  data_features['eid'].astype(str)\n",
    "data_features = data_features.set_index('eid', drop=False)\n",
    "#remove the samples for which the liver data is low quality\n",
    "data_features = data_features[data_features['Data_quality']!=np.nan]\n",
    "data_features = data_features.drop(\"Data_quality\", axis=1)\n",
    "#get rid of samples with NAs\n",
    "data_features = data_features.dropna()\n",
    "#list the samples' ids for which liver images are available\n",
    "all_files = os.listdir(dir_images)\n",
    "all_ids = [file.split(\".\", maxsplit=1)[0] for file in all_files]\n",
    "data_features = data_features.loc[all_ids]\n",
    "files = data_features.index.values\n",
    "#save the features\n",
    "data_features.to_csv(path_store + \"data_features_\" + version + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30336 validated image filenames.\n",
      "Found 3776 validated image filenames.\n",
      "Found 3712 validated image filenames.\n",
      "WARNING:tensorflow:From /home/al311/python_3.6.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#define data generators\n",
    "datagen_train=ImageDataGenerator(rescale=1./255.)\n",
    "datagen_test = ImageDataGenerator(rescale=1./255.)\n",
    "class_mode_train = \"raw\"\n",
    "class_mode_test = None\n",
    "\n",
    "#generate ids and image generators for train, val, test\n",
    "folds = ['train', 'val', 'test']\n",
    "data_features = pd.read_csv(path_store + \"data_features_\" + version + \".csv\")\n",
    "data_features['eid'] = data_features['eid'].apply(str)\n",
    "data_features[\"eid\"] = data_features[\"eid\"].apply(append_ext)\n",
    "data_features.set_index('eid', drop=False)\n",
    "ids = data_features.index.values.copy()\n",
    "np.random.shuffle(ids)\n",
    "percent_train = 0.8\n",
    "percent_val = 0.1\n",
    "n_discard = len(ids)%batch_size\n",
    "n_limit_train = math.ceil(len(ids)/batch_size*percent_train)*batch_size\n",
    "n_limit_val = math.ceil(len(ids)/batch_size*(percent_train+percent_val))*batch_size\n",
    "n_limit_test = len(ids)-n_discard\n",
    "\n",
    "#split IDs\n",
    "IDs={}\n",
    "IDs['train'] = ids[:n_limit_train]\n",
    "IDs['val'] = ids[n_limit_train:n_limit_val]\n",
    "IDs['test'] = ids[n_limit_val:n_limit_test]\n",
    "\n",
    "#compute values for scaling of Age\n",
    "fold = 'train'\n",
    "idx = np.where(np.isin(data_features.index.values, IDs['train']))[0]\n",
    "data_features_train = data_features.iloc[idx,:]\n",
    "Age_mean = data_features_train['Age'].mean()\n",
    "Age_std = data_features_train['Age'].std()\n",
    "\n",
    "#split data_features\n",
    "indices={}\n",
    "DATA_FEATURES = {}\n",
    "GENERATORS = {}\n",
    "STEP_SIZES = {}\n",
    "for fold in folds:\n",
    "    indices[fold] = np.where(np.isin(data_features.index.values, IDs[fold]))[0]\n",
    "    data_features_fold = data_features.iloc[indices[fold],:]\n",
    "    data_features_fold.to_csv(path_store + \"data_features_\" + fold + \".csv\")\n",
    "    data_features_fold['Age'] = (data_features_fold['Age']-Age_mean)/Age_std\n",
    "    if fold == 'test':\n",
    "        datagen=datagen_test\n",
    "        class_mode = class_mode_test\n",
    "    else:\n",
    "        datagen=datagen_train\n",
    "        class_mode = class_mode_train\n",
    "    \n",
    "    #define data generator\n",
    "    generator_fold = datagen.flow_from_dataframe(\n",
    "        dataframe=data_features_fold,\n",
    "        directory=dir_images,\n",
    "        x_col=\"eid\",\n",
    "        y_col=target,\n",
    "        color_mode=\"rgb\",\n",
    "        batch_size=batch_size,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        class_mode=\"raw\",\n",
    "        target_size=(image_size,image_size))\n",
    "    \n",
    "    #assign variables to their names\n",
    "    DATA_FEATURES[fold] = data_features_fold\n",
    "    GENERATORS[fold] = generator_fold\n",
    "    STEP_SIZES[fold] = generator_fold.n//generator_fold.batch_size\n",
    "\n",
    "#define the model\n",
    "x, base_model_input = generate_base_model(model_name=model_name, lam=lam, dropout_rate=dropout_rate, import_weights=import_weights)\n",
    "model = complete_architecture(x=x, input_shape=base_model_input, lam=lam, dropout_rate=dropout_rate)\n",
    "\n",
    "#initialise history\n",
    "HISTORY = initialize_history()\n",
    "\n",
    "#take subset to debunk\n",
    "#DATA_FEATURES['train'] = DATA_FEATURES['test']\n",
    "#GENERATORS['train'] = GENERATORS['test']\n",
    "#STEP_SIZES['train'] = STEP_SIZES['test']\n",
    "#n_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(re-)set the learning rate\n",
    "set_learning_rate(model, optimizer_name, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights to continue training\n",
    "path_weights = path_store + \"model_weights_\" + version + \".h5\"\n",
    "if continue_training & os.path.exists(path_weights):\n",
    "    print(\"loading previous model's weights\")\n",
    "    #load weights\n",
    "    model.load_weights(path_weights)\n",
    "    #load previous best performance\n",
    "    json_file = open(path_store + 'Performance_' + version + '.json', 'r')\n",
    "    best_perf = json_file.read()\n",
    "    json_file.close()\n",
    "    N_epochs = best_perf['N_epochs']\n",
    "    max_perf_val = best_perf[main_metric]['val']\n",
    "else:\n",
    "    N_epochs = 0\n",
    "    max_perf_val = -np.Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/al311/python_3.6.0/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "474/474 [==============================] - 637s 1s/step - loss: 63.1770 - R_squared: -61.1766 - root_mean_squared_error: 1.9518 - val_loss: 1.1078 - val_R_squared: -0.1328 - val_root_mean_squared_error: 1.0500\n",
      "Epoch 2/10\n",
      "474/474 [==============================] - 191s 404ms/step - loss: 0.8964 - R_squared: 0.0824 - root_mean_squared_error: 0.9372 - val_loss: 2.1768 - val_R_squared: -1.2105 - val_root_mean_squared_error: 1.4711\n",
      "Epoch 3/10\n",
      "474/474 [==============================] - 190s 400ms/step - loss: 0.8656 - R_squared: 0.1145 - root_mean_squared_error: 0.9211 - val_loss: 0.8045 - val_R_squared: 0.1835 - val_root_mean_squared_error: 0.8939\n",
      "Epoch 4/10\n",
      "474/474 [==============================] - 190s 400ms/step - loss: 0.7572 - R_squared: 0.2240 - root_mean_squared_error: 0.8651 - val_loss: 0.9469 - val_R_squared: 0.0414 - val_root_mean_squared_error: 0.9706\n",
      "Epoch 5/10\n",
      "474/474 [==============================] - 190s 401ms/step - loss: 0.6729 - R_squared: 0.3087 - root_mean_squared_error: 0.8169 - val_loss: 15.7717 - val_R_squared: -14.8501 - val_root_mean_squared_error: 3.3762\n",
      "Epoch 6/10\n",
      "474/474 [==============================] - 189s 399ms/step - loss: 0.6335 - R_squared: 0.3521 - root_mean_squared_error: 0.7926 - val_loss: 0.7383 - val_R_squared: 0.2480 - val_root_mean_squared_error: 0.8552\n",
      "Epoch 7/10\n",
      "474/474 [==============================] - 189s 398ms/step - loss: 0.6142 - R_squared: 0.3717 - root_mean_squared_error: 0.7801 - val_loss: 1024.8472 - val_R_squared: -1065.7995 - val_root_mean_squared_error: 31.3144\n",
      "Epoch 8/10\n",
      "474/474 [==============================] - 192s 404ms/step - loss: 0.6208 - R_squared: 0.3626 - root_mean_squared_error: 0.7841 - val_loss: 3.5363 - val_R_squared: -2.6268 - val_root_mean_squared_error: 1.5176\n",
      "Epoch 9/10\n",
      "474/474 [==============================] - 190s 401ms/step - loss: 0.5796 - R_squared: 0.4060 - root_mean_squared_error: 0.7580 - val_loss: 0.7418 - val_R_squared: 0.2441 - val_root_mean_squared_error: 0.8588\n",
      "Epoch 10/10\n",
      "474/474 [==============================] - 191s 403ms/step - loss: 0.5731 - R_squared: 0.4108 - root_mean_squared_error: 0.7536 - val_loss: 0.8781 - val_R_squared: 0.1096 - val_root_mean_squared_error: 0.9182\n",
      "474/474 [==============================] - 105s 222ms/step\n",
      "59/59 [==============================] - 13s 220ms/step\n",
      "58/58 [==============================] - 63s 1s/step\n",
      "N_epochs = 10\n",
      "Performance summary: \n",
      "{'R_squared': {'train': -0.7465012664845612, 'val': -0.7604058125766884, 'test': -0.587330158844827}, 'root_mean_squared_error': {'train': 1.3215308148435276, 'val': 1.3303205580355917, 'test': 1.268663593923233}}\n",
      "A better model was found in the middle of the epoch batch with validation R_squared = 0.2479790620884653\n",
      "Model's best weights for Age_Liver_DenseNet121_Adam_0.01_0.0_0.0_64 were saved.\n",
      "Epoch 1/10\n",
      "474/474 [==============================] - 200s 423ms/step - loss: 0.5778 - R_squared: 0.4057 - root_mean_squared_error: 0.7564 - val_loss: 0.8764 - val_R_squared: 0.1109 - val_root_mean_squared_error: 0.9321\n",
      "Epoch 2/10\n",
      "474/474 [==============================] - 196s 414ms/step - loss: 0.5907 - R_squared: 0.3933 - root_mean_squared_error: 0.7629 - val_loss: 0.6905 - val_R_squared: 0.2990 - val_root_mean_squared_error: 0.8284\n",
      "Epoch 3/10\n",
      "474/474 [==============================] - 196s 413ms/step - loss: 0.5705 - R_squared: 0.4131 - root_mean_squared_error: 0.7516 - val_loss: 1.1639 - val_R_squared: -0.1830 - val_root_mean_squared_error: 1.0765\n",
      "Epoch 4/10\n",
      "474/474 [==============================] - 195s 412ms/step - loss: 0.5544 - R_squared: 0.4286 - root_mean_squared_error: 0.7405 - val_loss: 2.0805 - val_R_squared: -1.1300 - val_root_mean_squared_error: 1.4383\n",
      "Epoch 5/10\n",
      "474/474 [==============================] - 193s 408ms/step - loss: 0.5384 - R_squared: 0.4481 - root_mean_squared_error: 0.7304 - val_loss: 0.7058 - val_R_squared: 0.2747 - val_root_mean_squared_error: 0.8369\n",
      "Epoch 6/10\n",
      "474/474 [==============================] - 196s 414ms/step - loss: 0.5353 - R_squared: 0.4513 - root_mean_squared_error: 0.7276 - val_loss: 0.7015 - val_R_squared: 0.2849 - val_root_mean_squared_error: 0.8339\n",
      "Epoch 7/10\n",
      "474/474 [==============================] - 193s 408ms/step - loss: 0.5332 - R_squared: 0.4514 - root_mean_squared_error: 0.7270 - val_loss: 0.9627 - val_R_squared: 0.0224 - val_root_mean_squared_error: 0.9777\n",
      "Epoch 8/10\n",
      "474/474 [==============================] - 193s 408ms/step - loss: 0.5086 - R_squared: 0.4780 - root_mean_squared_error: 0.7099 - val_loss: 1.5865 - val_R_squared: -0.6266 - val_root_mean_squared_error: 1.2565\n",
      "Epoch 9/10\n",
      "474/474 [==============================] - 194s 409ms/step - loss: 0.5039 - R_squared: 0.4814 - root_mean_squared_error: 0.7067 - val_loss: 0.7415 - val_R_squared: 0.2436 - val_root_mean_squared_error: 0.8594\n",
      "Epoch 10/10\n",
      "474/474 [==============================] - 194s 409ms/step - loss: 0.5032 - R_squared: 0.4843 - root_mean_squared_error: 0.7059 - val_loss: 1.1779 - val_R_squared: -0.2066 - val_root_mean_squared_error: 1.0825\n",
      "474/474 [==============================] - 103s 217ms/step\n",
      "59/59 [==============================] - 13s 218ms/step\n",
      "58/58 [==============================] - 12s 214ms/step\n",
      "N_epochs = 20\n",
      "Performance summary: \n",
      "{'R_squared': {'train': -0.7632248231851695, 'val': -0.7501563181165201, 'test': -0.7318255477914912}, 'root_mean_squared_error': {'train': 1.327842874742349, 'val': 1.3264421858488669, 'test': 1.3251496142322339}}\n",
      "A better model was found in the middle of the epoch batch with validation R_squared = 0.29896036043005475\n",
      "Model's best weights for Age_Liver_DenseNet121_Adam_0.01_0.0_0.0_64 were saved.\n",
      "Epoch 1/10\n",
      "474/474 [==============================] - 197s 417ms/step - loss: 0.5024 - R_squared: 0.4831 - root_mean_squared_error: 0.7050 - val_loss: 1.2239 - val_R_squared: -0.2535 - val_root_mean_squared_error: 1.1010\n",
      "Epoch 2/10\n",
      "474/474 [==============================] - 195s 412ms/step - loss: 0.5002 - R_squared: 0.4861 - root_mean_squared_error: 0.7037 - val_loss: 0.9338 - val_R_squared: 0.0535 - val_root_mean_squared_error: 0.9623\n",
      "Epoch 3/10\n",
      "474/474 [==============================] - 195s 412ms/step - loss: 0.4988 - R_squared: 0.4868 - root_mean_squared_error: 0.7025 - val_loss: 2.8377 - val_R_squared: -1.8923 - val_root_mean_squared_error: 1.6820\n",
      "Epoch 4/10\n",
      "474/474 [==============================] - 193s 407ms/step - loss: 0.5014 - R_squared: 0.4846 - root_mean_squared_error: 0.7047 - val_loss: 1.4288 - val_R_squared: -0.4592 - val_root_mean_squared_error: 1.1919\n",
      "Epoch 5/10\n",
      "432/474 [==========================>...] - ETA: 15s - loss: 0.5101 - R_squared: 0.4772 - root_mean_squared_error: 0.7102"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "while True:\n",
    "    history = model.fit_generator(generator=GENERATORS['train'],\n",
    "                    steps_per_epoch=STEP_SIZES['train'],\n",
    "                    validation_data=GENERATORS['val'],\n",
    "                    validation_steps=STEP_SIZES['val'],\n",
    "                    use_multiprocessing = True,\n",
    "                    epochs=n_epochs)\n",
    "    #compute performances\n",
    "    N_epochs += n_epochs\n",
    "    HISTORY = update_history(HISTORY, history)\n",
    "    plot_training(HISTORY, version)\n",
    "    PREDS, PERF = generate_predictions_and_performances(model, GENERATORS, STEP_SIZES)\n",
    "    print('N_epochs = ' + str(N_epochs))\n",
    "    print('Performance summary: ')\n",
    "    print(PERF)\n",
    "    if np.max(history.history['val_' + main_metric]) > max_perf_val:\n",
    "        print('A better model was found in the middle of the epoch batch with validation ' + main_metric + ' = ' + str(np.max(history.history['val_' + main_metric])))\n",
    "    if np.max(PERF[main_metric]['val']) > max_perf_val:\n",
    "        max_metric_val = np.max(PERF[main_metric]['val'])\n",
    "        save_model_weights(model, version)\n",
    "        to_save = PERF.copy()\n",
    "        to_save['version'] = version\n",
    "        to_save['N_epochs'] = N_epochs\n",
    "        json.dump(to_save, open(path_store + 'Performance_' + version + '.json','w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
